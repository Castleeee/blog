---
title: twitterçˆ¬è™«
date: 2022-07-06 15:18:34
category:
- çˆ¬è™«ğŸ•·
tag:
- pythonğŸ
- çˆ¬è™«ğŸ•·
---

<!-- more -->
<div align="center" style="font-size:1.4em;"><h2><strong> twitterçˆ¬è™«</strong></h2></div>


æ¨ç‰¹åçˆ¬æœºåˆ¶ç¨å¾®ä¸¥ä¸€äº›ï¼Œä½†æ˜¯ä¹Ÿä¸æ˜¯å¾ˆéš¾çªç ´ã€‚
è¿™æ¬¡çˆ¬è™«ä¸»è¦æ˜¯ç»ƒå¤šè¿›ç¨‹çš„ä½¿ç”¨  

## åˆ†æè¯·æ±‚
é¦–å…ˆæ‰¾è¯·æ±‚ï¼Œå¤§å°å¤ªç¦»è°±çš„éƒ½ä¸è¦ï¼Œ
![](./static/twitterçˆ¬è™«_images_1.png)

å› ä¸ºç½‘é¡µæ˜¯æ»‘åŠ¨çš„ï¼Œåº§æ¤…æ¯æ¬¡ä¸‹æ»‘çœ‹çœ‹ä»–åŠ è½½å“ªäº›ï¼Œæ‰¾è¿™äº›é‡Œé¢çš„æ•°æ®ã€‚  
ä¸€å¼€å§‹é”å®šçš„æ˜¯all.jsonå’Œguid.jsonåæ¥å‘ç°guid.jsonè¿”å›çš„æ˜¯è¾¹æ çš„æ¨èï¼Œallè¿”å›çš„æ˜¯ä¸€äº›ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯ä¹Ÿå°å°çš„è¯¯å¯¼äº†æˆ‘  
åæ¥æ‰¾åˆ°æ˜¯è¿™ä¸ª  
![1200](./static/twitterçˆ¬è™«_images_2.png)<br/>
![900](./static/twitterçˆ¬è™«_images_3.png)

å‘ç°æ¯æ¬¡æŸ¥è¯¢å˜åŒ–çš„éƒ½æ˜¯è¿™ä¸ª**cursor**ï¼Œå› ä¸ºä¹‹å‰allé‡Œé¢æœ‰å¾ˆå¤šä¿¡æ¯ï¼Œæˆ‘ä»¥ä¸ºè¿™ä¸ªcursroæ˜¯è®¡ç®—å‡ºæ¥çš„ï¼Œæ‰€ä»¥å°±åœ¨jsé‡Œé¢é€†å‘æ‰¾ç‰‡æ®µï¼Œéå¸¸éš¾æ‰¾ï¼Œæœ€åæ‰¾åˆ°å¤´å‘ç°ï¼Œæ˜¯è¿™æ¬¡çš„jsoné‡Œé¢å¸¦ç€ä¸‹æ¬¡çš„æ ‡  

![600](./static/twitterçˆ¬è™«_images_4.png)

è¿”å›çš„jsoné‡Œé¢åŸºæœ¬ä¸ŠåŒ…å«äº†æ‰€æœ‰çš„å†…å®¹ã€‚


## é‡å‘è¯·æ±‚
ç›´æ¥åœ¨æµè§ˆå™¨é‡Œé¢é‡å‘è¿”å›403ï¼Œä»¥ä¸ºæ˜¯æœåŠ¡å™¨æœ‰åçˆ¬ï¼Œç„¶åæŠŠè¿™ä¸ªè¯·æ±‚å¤åˆ¶curlæ”¾åˆ°postmané‡Œé¢å»ï¼Œé‡å‘å¾—åˆ°äº†æ•°æ®  
çŒœæµ‹æ˜¯æœ‰äº›è¯·æ±‚å¤´æ²¡å¸¦ä¸Šï¼Œæ¨ç‰¹è‡ªå·±çš„è¯·æ±‚å¤´é‡Œé¢æœ‰å¥½å‡ ä¸ªè‡ªå®šä¹‰çš„å‚æ•°ï¼Œä¼°è®¡æ˜¯ä¸å¸¦ä¸Šå°±å‘ä¸äº†  
![900](./static/twitterçˆ¬è™«_images_5.png)

ä¹‹ååœ¨pythoné‡Œè¯·æ±‚å°±å¯ä»¥äº†ï¼Œå‚æ•°å¾ˆå¤šé…èµ·æ¥éº»çƒ¦  
æ¨æ–‡å®é™…ä¸Šåœ¨é‡Œé¢å°±æœ‰äº†ï¼Œä½†æ˜¯ä¸ºäº†è¯¦ç»†ä¸€ç‚¹ï¼Œé¡ºæ‰‹æŠŠè¯„è®ºç¬¬ä¸€é¡µä¹Ÿçˆ¬ä¸€ä¸‹  
æ¥å£æ˜¯TweetsDetail  
::: details Click to see more

```python
import requests

cookies = {
    ...
}

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:101.0) Gecko/20100101 Firefox/101.0',
    'Accept': '*/*',
    'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
    # 'Accept-Encoding': 'gzip, deflate, br',
    'content-type': 'application/json',
    'x-twitter-auth-type': 'OAuth2Session',
    'x-twitter-client-language': 'zh-cn',
    'x-twitter-active-user': 'yes',
    'x-csrf-token': '1b73c3ff3f8905027c09c8....1c1c3d6a65c07e4b67f2f67c715a0728ccd2bb445956c',
    'Sec-Fetch-Dest': 'empty',
    'Sec-Fetch-Mode': 'cors',
    'Sec-Fetch-Site': 'same-origin',
    'authorization': 'Bearer AAAAAAAAsdasdk8LF81IU...jCpTnA',
    'Referer': 'https://twitter.com/taylorswift13',
    'Connection': 'keep-alive',
}

params = {
    'variables': '{"userId":"17919972","count":40,"cursor":"HBaAwLTRytqKnSIAAA==","includePromotedContent":true,"withQuickPromoteEligibilityTweetFields":true,"withSuperFollowsUserFields":true,"withDownvotePerspective":false,"withReactionsMetadata":false,"withReactionsPerspective":false,"withSuperFollowsTweetFields":true,"withVoice":true,"withV2Timeline":true}',
    'features': '{"dont_mention_me_view_api_enabled":true,"interactive_text_enabled":true,"responsive_web_uc_gql_enabled":false,"vibe_tweet_context_enabled":false,"responsive_web_edit_tweet_api_enabled":false,"standardized_nudges_misinfo":false,"responsive_web_enhance_cards_enabled":false}',
}

response = requests.get('https://twitter.com/i/api/graphql/LeRJx69CS_6El2rAG0HQ9g/UserTweets', params=params, cookies=cookies, headers=headers)
```
è¿è¡Œç»“æœ
```
jsonå¾ˆé•¿å¾ˆé•¿ï¼Œæ²¡äººä¼šçœ‹çš„
```
:::

## è®¾è®¡çˆ¬è™«
### ä»£ç 
è€ä¸‰æ ·ï¼Œè¯·æ±‚ï¼Œå¤„ç†ï¼ŒæŒä¹…åŒ–ï¼Œç¨‹åºå¾ˆé•¿çš„æ—¶å€™å°±éœ€è¦å†™ä¸€ä¸ªè°ƒåº¦å™¨  
ä»£ç å’Œç»“æ„å¥‰ä¸Š  
è¿™å››ä¸ªæ–‡ä»¶éƒ½æ˜¯åŒä¸€ç›®å½•ä¸‹çš„  
conf.py é…ç½®
::: details Click to see more

```python
# -*-coding:utf-8-*-  
# SettingCode here  
  
COOKIES = {  
    ...
}  
  
HEADERS = {  
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:101.0) Gecko/20100101 Firefox/101.0',  
    'Accept': '*/*',  
    'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',  
    'content-type': 'application/json',  
    # æ¨ç‰¹è‡ªå·±æ•´çš„header ä¸é…ç½®å°±403  
    'x-twitter-auth-type': 'OAuth2Session',  
    'x-twitter-client-language': 'zh-cn',  
    'x-twitter-active-user': 'yes',  
    # csrf token å¯èƒ½ä¼šå˜  
    'x-csrf-token': '1b73c3ff3f8905027c09c83f2529sdasdasbc45ed03d998458ced11b1c1c3d6a65c07e4b67f2f67c715a0728ccd2bb445956c',  
    'Sec-Fetch-Dest': 'empty',  
    'Sec-Fetch-Mode': 'cors',  
    'Sec-Fetch-Site': 'same-origin',  
    # è¿™ä¸ªå­—æ®µä¸çŸ¥é“å¹²å˜›çš„ï¼Œä½†æ˜¯å¾ˆæ˜æ˜¾ä¸æ˜¯è‡ªå¸¦çš„  
    'authorization': 'Bearer AAAAAAAAAAAAAAAsdsadsadZz4puTs%3D1Zv7ttfk8LF81IUq16cHjhLTvJu4FA33AGWWjCpTnA',  
    'Referer': "",  
    'Connection': 'keep-alive',  
}  
PROXY = {  
    'http': 'http://127.0.0.1:7890',  
    'https': 'http://127.0.0.1:7890',  
}  
# çˆ¬å“ªä¸ªç”¨æˆ·  
USERID = 17919972  
CRAWLED_PIN = 0  # åˆ¶å®šæ˜¯å¦è¢«çˆ¬è¿‡  
TIMELINE_SLEEP = 15  # æ¯ä¸ªtimelineçº¿ç¨‹æ¯æ¬¡è¯·æ±‚é—´éš”æ—¶é—´  
DETAIL_SLEEP = 15  # æ¯ä¸ªæ¨æ–‡è¯·æ±‚é—´éš”
```
è¿è¡Œç»“æœ
```

```
:::
scheduler.py ä¸»ç¨‹åºï¼Œè°ƒåº¦
::: details Click to see more

```python
# -*-coding:utf-8-*-  
# SettingCode here  
__author__ = "a_little_rubbish"  
__date__ = "2022/7/5 15:06"  
  
# import your model here  
  
import time  
from contextlib import contextmanager  
  
from queue import Empty  
import multiprocessing  
from multiprocessing import Manager  
  
import timeline_spider  
import detail_spider  
import persistence  
from conf import *  
  
  
def is_done(status_table, p_type):  
    """  
    æ£€æŸ¥æŸä¸€ç±»çº¿ç¨‹æ˜¯å¦å…¨éƒ½ç»“æŸäº†  
    :param status_table: çŠ¶æ€è¡¨  
    :param p_type: çº¿ç¨‹ç±»å‹  
    :return:  
    """    done = []  
    print("ProcessStatus:", status_table)  
    for k, v in status_table[p_type].items():  # éå†çŠ¶æ€è¡¨  
  
        if v == 0:  # æ·»åŠ çŠ¶æ€  
            done.append(True)  
        else:  
            done.append(False)  
    return all(done)  # å…¨å®Œæˆå°±è¿”å›Tuee  
  
  
@contextmanager  
def process_info_logger(status_table, p_type, p_name):  
    """  
    è¿›ç¨‹å‡½æ•°çŠ¶æ€ç®¡ç†  
    :param status_table: çŠ¶æ€è¡¨  
    :param p_type: å“ªç±»è¿›ç¨‹  
    :param p_name: è¿›ç¨‹åï¼Œåˆå§‹åŒ–ç”¨  
    :return:  
    """    temp = status_table[p_type]  # è¿›ç¨‹åµŒå¥—å­—å…¸éœ€è¦ä¸­é—´è½¬ä¸€ä¸‹èµ‹å€¼  
    temp[p_name] = 1  
    status_table[p_type] = temp  
    print("%s starting..." % p_name)  
    yield  
    temp = status_table[p_type]  # è¿›ç¨‹ç»“æŸ  
    temp[p_name] = 0  
    status_table[p_type] = temp  
    print("%s has done" % p_name)  
  
  
def timeline_crawler(status_table, detail_Q):  
    """  
    çˆ¬æ—¶é—´çº¿çš„è¿›ç¨‹å‡½æ•°  
    åªèƒ½èµ·ä¸€ä¸ªï¼Œå¤šç”¨æˆ·æ‰èƒ½å¤šä¸ªï¼Œå› ä¸ºä¸èƒ½åˆ‡ç‰‡åªèƒ½ä¸‹æ»‘  
    """    name = multiprocessing.current_process().name  
    with process_info_logger(status_table, "timeline", name):  
        cursor = ""  # ç¬¬ä¸€æ¬¡ä¸ºç©º  
        while True:  
            try:  
                result = timeline_spider.crawl(cursor)  
            except Exception as e: # å‡ºç°é”™è¯¯å°±è¿”å›  
                print(e)  
                break  
            if result == 0:  # è¿™ä¸ªäººçš„å·²ç»çˆ¬å®Œäº†  
                break  
            for i in result["detail_list"]:  
                detail_Q.put(i)  # ç»“æœåŠ è¿›å»  
            cursor = result["cursor"]  # è·å¾—ä¸‹ä¸€æ¬¡çš„æ ‡  
  
            time.sleep(TIMELINE_SLEEP)  # æ§åˆ¶é¢‘ç‡  
  
  
def detail_crawler(status_table, detail_Q, ready_data_Q):  
    """  
    çˆ¬æ¨æ–‡ç»†èŠ‚çš„è¿›ç¨‹å‡½æ•°,æ”¾åˆ°ready_data_Q  
    """    name = multiprocessing.current_process().name  
  
    with process_info_logger(status_table, "detail", name):  
        while True:  
            try:  
                tw_id = detail_Q.get(timeout=3)  
  
            except Empty as e:  
                if is_done(status_table, "timeline"):  
                    break  
                continue            except Exception as e:  
                print(e)  
                break  
            ready_data = detail_spider.crawl(tw_id)  
            ready_data_Q.put(ready_data)  
  
            time.sleep(DETAIL_SLEEP)  # æ§åˆ¶é¢‘ç‡  
  
  
def persist(status_table, ready_data_Q):  
    """  
    ä»ready_data_Qæ‹¿æ•°æ®æŒä¹…åŒ–ï¼Œå­˜æ•°æ®åº“  
    """    name = multiprocessing.current_process().name  
  
    with process_info_logger(status_table, "persist", name):  
        while True:  
            try:  
                data = ready_data_Q.get(timeout=3)  
            except Empty as e:  
                if is_done(status_table, "detail"):  
                    break  
                continue            persistence.persist_sql(data)  
  
  
def main():  
    """  
    ä¸»å‡½æ•°  
    """    pool = []  
    for i in range(1, 2):  # çˆ¬æ—¶é—´çº¿çš„çº¿ç¨‹  
        p = multiprocessing.Process(target=timeline_crawler,  
                                    args=(status_table, detail_Q),  
                                    name='timelineCrawlr%d' % i)  
        pool.append(p)  
  
    for i in range(1, 3):  # çˆ¬æ¨æ–‡çš„çº¿ç¨‹  
        p = multiprocessing.Process(target=detail_crawler,  
                                    args=(status_table, detail_Q, ready_data_Q),  
                                    name='detailCrawler%d' % i)  
        pool.append(p)  
  
    for i in range(1, 3):  # æŒä¹…åŒ–çº¿ç¨‹  
        p = multiprocessing.Process(target=persist,  
                                    args=(status_table, ready_data_Q),  
                                    name='persister%d' % i)  
        pool.append(p)  
  
    for i in pool:  
        i.start()  
    for i in pool:  
        i.join()  
  
  
if __name__ == '__main__':  
    status_table = Manager().dict({"timeline": {}, "detail": {}, "persist": {}})  # è¿›ç¨‹çŠ¶æ€è¡¨,1æ˜¯è¿è¡Œï¼Œ0æ˜¯è¿è¡Œç»“æŸ  
    detail_Q = Manager().Queue(maxsize=200)  # æ¯ä¸ªtweetçš„idé˜Ÿåˆ—  
    ready_data_Q = Manager().Queue(maxsize=200)  # å°±ç»ªæ•°æ®é˜Ÿåˆ—\  
  
    persistence.init_table()  # åˆå§‹åŒ–æ•°æ®åº“  
    main()
```
è¿è¡Œç»“æœ
```
2022-07-06 15:11:52,653 INFO sqlalchemy.engine.Engine SELECT DATABASE()
2022-07-06 15:11:52,653 INFO sqlalchemy.engine.Engine [raw sql] {}
2022-07-06 15:11:52,666 INFO sqlalchemy.engine.Engine SELECT @@sql_mode
2022-07-06 15:11:52,667 INFO sqlalchemy.engine.Engine [raw sql] {}
2022-07-06 15:11:52,667 INFO sqlalchemy.engine.Engine SELECT @@lower_case_table_names
2022-07-06 15:11:52,667 INFO sqlalchemy.engine.Engine [raw sql] {}
2022-07-06 15:11:52,680 INFO sqlalchemy.engine.Engine BEGIN (implicit)
2022-07-06 15:11:52,681 INFO sqlalchemy.engine.Engine SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = %(table_schema)s AND table_name = %(table_name)s
2022-07-06 15:11:52,681 INFO sqlalchemy.engine.Engine [generated in 0.00012s] {'table_schema': 'twitter', 'table_name': 'posts'}
2022-07-06 15:11:52,768 INFO sqlalchemy.engine.Engine 
DROP TABLE posts
2022-07-06 15:11:52,768 INFO sqlalchemy.engine.Engine [no key 0.00022s] {}
2022-07-06 15:11:52,958 INFO sqlalchemy.engine.Engine COMMIT
2022-07-06 15:11:52,964 INFO sqlalchemy.engine.Engine BEGIN (implicit)
2022-07-06 15:11:52,964 INFO sqlalchemy.engine.Engine SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = %(table_schema)s AND table_name = %(table_name)s
2022-07-06 15:11:52,964 INFO sqlalchemy.engine.Engine [cached since 0.2835s ago] {'table_schema': 'twitter', 'table_name': 'posts'}
2022-07-06 15:11:52,975 INFO sqlalchemy.engine.Engine 
CREATE TABLE posts (
	id INTEGER NOT NULL AUTO_INCREMENT, 
	main_posts TEXT NOT NULL, 
	reply TEXT, 
	PRIMARY KEY (id)
)


2022-07-06 15:11:52,975 INFO sqlalchemy.engine.Engine [no key 0.00009s] {}
2022-07-06 15:11:53,110 INFO sqlalchemy.engine.Engine COMMIT
detailCrawler2 starting...
detailCrawler1 starting...
persister2 starting...
persister1 starting...
timelineCrawlr1 starting...
status 200 crawing timelineSlice  
extracting timeline data  
ProcessStatus: ProcessStatus: {'timeline': {'timelineCrawlr1': 1}, 'detail': {'detailCrawler2': 1, 'detailCrawler1': 1}, 'persist': {'persister2': 1, 'persister1': 1}}
{'timeline': {'timelineCrawlr1': 1}, 'detail': {'detailCrawler2': 1, 'detailCrawler1': 1}, 'persist': {'persister2': 1, 'persister1': 1}}
status 200 crawing tweets 1459023801480474632
status 200 crawing tweets 1540382753677627393
extracting tweets data 1540382753677627393 
extracting tweets data 1459023801480474632 
inserting data {'posts': ['Iâ€™m absolutely terrified that this is where we are - that after so many decades of people fighting for womenâ€™s rights to their own bodies, todayâ€™s decision has stripped us of that.'], 'reply': ['@taylorswift13 A babyâ€™s body is *not* your body. Let them live. Abortion is violence.', '@taylorswift13 why on earth does a gun have more protection in this country than i do??', '@taylorswift13 You may have lost a fan unless Iâ€™m misunderstanding you. Abortion is murder. Contraceptives should be used if youâ€™re having sex and canâ€™t handle/donâ€™t want to have a child. In certain cases (rape, womanâ€™s health etcâ€¦) an abortion should be an optionâ€¦', '@taylorswift13 I donâ€™t understand why everyone is upsetâ€¦ if you donâ€™t want a baby just donâ€™t get pregnantâ€¦ right? And if you do, why just remove the existence of the fetus inside you, that you were once yourself, erasing their entire future and existence because you donâ€™t want them? \U0001fae5', '@taylorswift13 Kindly explain why women who fight so hard for their own bodies find it so easy to destroy someone elseâ€™s (a babyâ€™s, no less). Take your time, Iâ€™ll wait. (By the way, hundreds of babies were scheduled to die today, but now wonâ€™t. Does their reprieve make you happy or sad?)', '@taylorswift13 So disappointed in myself that I used to look up to you as a teenager! Taylor, your mother chose life, and chose to have you. If it wasnâ€™t for her choosing life, you wouldnâ€™t even have rights.', '@taylorswift13 Iâ€™m soooo disappointed to hear you say this â€¦ nobody should be able to choose to end someone elseâ€™s life.  No matter how old the person is, (fetus, 5 yearâ€™s old or 40 years old) itâ€™s a living being. The womenâ€™s choice was to have sex, so they need to deal with the consequences.', '@taylorswift13 @BlueberryGabe Iâ€™m terrified of a centralized government dictating what all of us have to believe. I am less terrified now that the states have the individual power to decide what for their local cultures. If youâ€™re really that worried take a morning-after pill. Does the same thing.', '@taylorswift13 @Kenshiro73 Thatâ€™s just the start. They wonâ€™t stop there.  \n\nhttps://t.co/h8xzUX5FPx']}
inserting data {'posts': ['It never would have been possible to go back &amp; remake my previous work, uncovering lost art &amp; forgotten gems along the way if you hadnâ€™t emboldened me. Red is about to be mine again, but it has always been ours. Now we begin again. Red (my version) is out\n\nhttps://t.co/ZUAWDuv4jL https://t.co/Ji26KdOlWQ', 'https://pbs.twimg.com/media/FD9-aXKWYAATZTm.jpg', 'https://pbs.twimg.com/media/FD9-aXIWYBI4DP6.jpg'], 'reply': ['@taylorswift13 i just wanna say that the very first night is one of the best vauIt track ever ğŸ˜ thanks for this b0p songğŸ˜¿â¤ï¸ \n https://t.co/Z86fmElt6A', '@taylorswift13 yassss speak now tv announcement!!\nhttps://t.co/S6U0tbJBCi', '@taylorswift13 ei tweeta aÃ­ https://t.co/SasFRYPgau', "@taylorswift13 OMG it's here ğŸ¥ºâ¤ï¸", "@taylorswift13 Best singer in the world is miss Taylor swift lady's and gentlemen love voodoo XX https://t.co/1iI3DjRoxd", "@taylorswift13 Tomorrow (Dhul-Hijjah 1st / July 1st) is Muslims valentine day due to the anniversary of a shining marriage of  a sacred couple Ali &amp; Fatemah. Their brilliant love is still an ideal one for marriage of many muslims and that's the reason they mostly have a great family.\n#pure_love", '@taylorswift13 Why do you have tour dates in venues that are 21 and older? Youâ€™re cutting off a lot of fans.', '@taylorswift13 A red rose grew up out of ice frozen ground, with no one around to tweet it. https://t.co/nB7KX5SzJH', '@taylorswift13 World s best singer ever. Miss Taylor swift best album ever sold red album  bye miss Taylor swift love voodoo XX my queen my love for ever XX https://t.co/slWOh6Q9xr']}
ProcessStatus: {'timeline': {'timelineCrawlr1': 1}, 'detail': {'detailCrawler2': 1, 'detailCrawler1': 1}, 'persist': {'persister2': 1, 'persister1': 1}}
ProcessStatus: {'timeline': {'timelineCrawlr1': 1}, 'detail': {'detailCrawler2': 1, 'detailCrawler1': 1}, 'persist': {'persister2': 1, 'persister1': 1}}
ProcessStatus: {'timeline': {'timelineCrawlr1': 1}, 'detail': {'detailCrawler2': 1, 'detailCrawler1': 1}, 'persist': {'persister2': 1, 'persister1': 1}}
ProcessStatus: {'timeline': {'timelineCrawlr1': 1}, 'detail': {'detailCrawler2': 1, 'detailCrawler1': 1}, 'persist': {'persister2': 1, 'persister1': 1}}
ProcessStatus: ProcessStatus: {'timeline': {'timelineCrawlr1': 1}, 'detail': {'detailCrawler2': 1, 'detailCrawler1': 1}, 'persist': {'persister2': 1, 'persister1': 1}}
{'timeline': {'timelineCrawlr1': 1}, 'detail': {'detailCrawler2': 1, 'detailCrawler1': 1}, 'persist': {'persister2': 1, 'persister1': 1}}
ProcessStatus: ProcessStatus: {'timeline': {'timelineCrawlr1': 1}, 'detail': {'detailCrawler2': 1, 'detailCrawler1': 1}, 'persist': {'persister2': 1, 'persister1': 1}}
{'timeline': {'timelineCrawlr1': 1}, 'detail': {'detailCrawler2': 1, 'detailCrawler1': 1}, 'persist': {'persister2': 1, 'persister1': 1}}
ProcessStatus: {'timeline': {'timelineCrawlr1': 1}, 'detail': {'detailCrawler2': 1, 'detailCrawler1': 1}, 'persist': {'persister2': 1, 'persister1': 1}}
ProcessStatus: {'timeline': {'timelineCrawlr1': 1}, 'detail': {'detailCrawler2': 1, 'detailCrawler1': 1}, 'persist': {'persister2': 1, 'persister1': 1}}
status 200 crawing timelineSlice "cursor":"HBaOgKPFjJfCwCcAAA==", 
extracting timeline data HBaOgKPFjJfCwCcAAA== 
status 200 crawing tweets 1540183720845533185
extracting tweets data 1540183720845533185 
inserting data {'posts': ['About a year &amp; half ago I wrote a song about the story of a girl who always lived on the outside, looking in. Figuratively &amp; literally. The juxtaposition of her loneliness &amp; independence. Her curiosity &amp; fear all tangled up. Her persisting gentleness &amp; the worldâ€™s betrayal of it. https://t.co/2o1z8Hrht6', 'https://pbs.twimg.com/ext_tw_video_thumb/1540183648468623360/pu/img/xHEqJJY-HYNZqQTZ.jpg'], 'reply': ['I wrote this one alone in the middle of the night and then @AaronDessner and I meticulously worked on a sound that we felt would be authentic to the moment when this story takes place. I made a wish that one day you would hear it. â€˜Carolinaâ€™ is out now ğŸ¥º \nhttps://t.co/2xqE0fEr10', '@taylorswift13 Thank for the song Taylor! Now please leave folkmore forest so that we can get TS10 yeah?', '@taylorswift13 Wen a song about leprechauns? @Leprechaun_Wtf #Leprechauntownwtf #LeprechaunTakeOver https://t.co/7aus6zadZd', '@taylorswift13 @jeonality @worthwhilefigh1 this is what u get for being optimist ;)', '@taylorswift13 @Leprechaun_Wtf got talent ms swift check out our theme song by the one and only @LiggyNFT', '@taylorswift13 my favorite tagline to see nowadays ğŸ¤ https://t.co/PGjxahqHx6', '@taylorswift13 Get in Swifties, weâ€™re going to the swamp! #Carolina https://t.co/vOVYgXqNGo', '@taylorswift13 Taylor only used instruments that were available before 1953 and Taylor recorded the song in one take like they did back thenâ€¦ no one is as extra as Ms. Swift https://t.co/4TPF4YWNUV']}
status 200 crawing tweets 1529295926057123841
extracting tweets data 1529295926057123841 
inserting data {'posts': ['Filled with rage and grief, and so broken by the murders in Uvalde. By Buffalo, Laguna Woods and so many others. By the ways in which we, as a nation, have become conditioned to unfathomable and unbearable heartbreak. Steveâ€™s words ring so true and cut so deep.'], 'reply': ['@taylorswift13 de la nada no', "@taylorswift13 We should never get used on violence, no matter how many times we turn on the news just to see it all again. It is not just a number, is someone's daughter as you said in epiphany. As a health worker that deals with mothers in grief daily it breas my heart for each family.", "@taylorswift13 As a Canadian, I hate to say it, but nothing will change. Absolutely nothing.  Your government officials offer thoughts and prayers every single time, as if that's going to fix things.  I guarantee someone has said there should have been an armed guard at the school.  ğŸ¤¬ğŸ¤¬ğŸ¤¬", '@taylorswift13 If there are 50 senators refusing to vote, how are they still in office (if, essentially, they are refusing to do their job?) \n\nSomeone please (nicely) educate me', '@taylorswift13 What this shows is not the relationship between mental illness and gun violence, but between virtually unregulated access to assault weapons and gun violence. This has to change because it has to. Right now. https://t.co/e1cWDwkpOC', '@taylorswift13 I canâ€™t imagine what itâ€™s like to be a parent in the US. Knowing that every day you send your kids to school, it might be the last time. Every time you kiss them in the morning, it might be the last kiss. I donâ€™t live in the US, I canâ€™t affect its politics. But US lawmakers can!', '@taylorswift13 Thank you for speaking out on this, itâ€™s feeling like nothing will ever change &amp; this will just keep happening regularly because thatâ€™s the reality we live in ğŸ’”', '@taylorswift13 We are all so tired, sad &amp; angry. I was a teen when Columbine happened. 23 yrs, weâ€™ve had 23 yrs to do SOMETHING. And yet weâ€™ve done NOTHING. Elected officials have the ability to do SOMETHING. Remember that. Weâ€™ve had ENOUGH. Our country stands alone in #s of mass shootings. 1/2', '@taylorswift13 swifties pls donâ€™t make this ab u', '@taylorswift13 Taylor. Or Taylor social media person. Girl where are you on Roe v Wade??? I thought maybe you were just not getting out there on any current events or legislation, but this tweet? So youâ€™re out here. Say something about abortion. Or tell us youâ€™re pro life.']}

...
...
...
```
:::

timeline_spider.py çˆ¬æ—¶é—´çº¿
::: details Click to see more

```python
# -*-coding:utf-8-*-  
# SettingCode here  
__author__ = "a_little_rubbish"  
__date__ = "2022/7/5 09:11"  
  
# import your model here  
import time  
from pprint import pprint  
  
import requests  
from requests.exceptions import ProxyError  
  
from conf import *  
  
# your class&function here  
  
# userId ä¼šæ ¹æ®ä¸åŒç”¨æˆ·å˜,%så¤„æ˜¯cursor  
PARAMS_ = {  
    'variables': '{"userId":"%d","count":40,%s"includePromotedContent":true,"withQuickPromoteEligibilityTweetFields":true,"withSuperFollowsUserFields":true,"withDownvotePerspective":false,"withReactionsMetadata":false,"withReactionsPerspective":false,"withSuperFollowsTweetFields":true,"withVoice":true,"withV2Timeline":true}',  
    'features': '{"dont_mention_me_view_api_enabled":true,"interactive_text_enabled":true,"responsive_web_uc_gql_enabled":false,"vibe_tweet_context_enabled":false,"responsive_web_edit_tweet_api_enabled":false,"standardized_nudges_misinfo":false,"responsive_web_enhance_cards_enabled":false}',  
}  
  
TIMELINE_URL = "https://twitter.com/i/api/graphql/LeRJx69CS_6El2rAG0HQ9g/UserTweets"  
  
  
def crawl(cursor, userId=USERID):  
    """  
    å¤–éƒ¨è°ƒç”¨è¿™ä¸ªå‡½æ•°  
    è§£æä¸€æ¬¡timelineçš„cursor  
    :return: {detailList:int list,cursor:str }  
    """    raw_json = download(cursor, userId)  
    if raw_json == -1:  
        return "ERROR"  
    result = extract(raw_json, cursor)  
    return result  
  
  
def extract(data, cursor):  
    """  
    å¤„ç†json  
    :param cursor: ä¸ºäº†æ‰“å°æ—¥å¿—å¥½çœ‹  
    :param data raw_json    :return: {detail_list:int list,cursor:str }  
    """    global CRAWLED_PIN  
    print("extracting timeline data %s " % cursor)  
  
    # å–æ•°æ®ï¼Œæ²¡ä»€ä¹ˆå¥½è¯´çš„ï¼Œç»“æ„é•¿ä»€ä¹ˆæ ·å°±æ€ä¹ˆå–  
    detail_list = []  
    cursor_next = ""  
    instructions = data["data"]["user"]["result"]["timeline_v2"]["timeline"]["instructions"]  
  
    entries = []  
    for i in instructions:  
        if i["type"] == "TimelineAddEntries":  # æ—¶é—´çº¿id  
            entries = i["entries"]  
            if len(entries) == 2:  
                return 0  # ä»£è¡¨çº¿ç¨‹å·²ç»çˆ¬å®Œï¼Œå‘Šè¯‰å¤–é¢ç»“æŸ  
        if i["type"] == "TimelinePinEntry":  # æ‹¿ç½®é¡¶æ¨çš„id  
            if CRAWLED_PIN == 0:  # æ¯æ¬¡è¯·æ±‚éƒ½ä¼šå¸¦ç½®é¡¶ï¼Œçˆ¬ä¸€æ¬¡å°±è¡Œ  
                entry_id = i["entry"]["entryId"].split("-")<br/>[1]  
                detail_list.append(entry_id)  
                CRAWLED_PIN = 1  
  
    for entry in entries:  
  
        entry_id = entry["entryId"]  
        if entry_id.startswith(("promotedTweet", "whoToFollow", "cursor-top")):  # æ¨å¹¿å’Œå»ºè®®å…³æ³¨éƒ½ä¸è¦  
            continue  
        elif entry_id.startswith(("tweet", "homeConversation")):  
            entry_id = entry_id.split("-")<br/>[1]  
            detail_list.append(entry_id)  
        elif entry_id.startswith("cursor-bottom"):  
            cursor_next = entry["content"]["value"]  
  
    return {"detail_list": detail_list, "cursor": cursor_next}  
  
  
def download(cursor, userId):  
    """  
    è¯·æ±‚å¹¶åˆ¤æ–­è¿æ¥çŠ¶æ€å¤„ç†  
    :param cursor userId    :return: raw_json  
    """    if cursor != "":  
        cursor = '"cursor":"%s",' % cursor  # æ§åˆ¶ä¸‹æ‹‰åŠ è½½ä»€ä¹ˆçš„å‚æ•°ï¼Œé¦–æ¬¡è¯·æ±‚æ²¡æœ‰ï¼Œä¹‹åæ¯æ¬¡ä¸‹æ‹‰åŠ è½½éƒ½ä¼šåœ¨jsoné‡Œå­˜æ”¾ä¸‹ä¸€æ¬¡çš„ï¼Œæ¯æ¬¡éƒ½ä¼šåˆ·æ–°  
    PARAMS = PARAMS_.copy()  
    PARAMS["variables"] = PARAMS["variables"] % (userId, cursor)  
    while True:  
        try:  
            response = requests.get(TIMELINE_URL, proxies=PROXY, params=PARAMS, cookies=COOKIES, headers=HEADERS)  
        except ProxyError or ConnectionResetError:  
            print("timeline:ConnectionResetError")  
            time.sleep(20)  
            continue  
        break  
    if response.status_code != 200:  # è¯·æ±‚é”™è¯¯å°±å‘ä¸Šä¼ é€’  
        print("error %d when crawing timelineSlice %s " % (response.status_code, cursor))  
        return -1  
  
    print("status %d crawing timelineSlice %s " % (response.status_code, cursor))  
    return dict(response.json())  
  
  
if __name__ == "__main__":  
    crawl("HBaOgKPFjJfCwCcAAA==")
```
è¿è¡Œç»“æœ
```
status 200 crawing timelineSlice "cursor":"HBaOgKPFjJfCwCcAAA==", 
extracting timeline data HBaOgKPFjJfCwCcAAA== 
```
:::
detail_spider.py çˆ¬ç»†èŠ‚
::: details Click to see more

```python
# -*-coding:utf-8-*-  
# SettingCode here  
__author__ = "a_little_rubbish"  
__date__ = "2022/7/5 15:06"  
  
import time  
  
import requests  
from requests.exceptions import ProxyError  
  
from conf import *  
  
PARAMS_ = {  # ä¸èƒ½ç›´æ¥ä¿®æ”¹  
    'variables': '{"focalTweetId":"%d","with_rux_injections":false,"includePromotedContent":true,"withCommunity":true,'  # è¿™ä¸ªfocalTweetIdæ˜¯æ¨æ–‡çš„è¯¦æƒ…  
                 '"withQuickPromoteEligibilityTweetFields":true,"withBirdwatchNotes":true,'  
                 '"withSuperFollowsUserFields":true,"withDownvotePerspective":false,"withReactionsMetadata":false,'                 '"withReactionsPerspective":false,"withSuperFollowsTweetFields":true,"withVoice":true,'                 '"withV2Timeline":true}',  
    'features': '{"dont_mention_me_view_api_enabled":true,"interactive_text_enabled":true,'  
                '"responsive_web_uc_gql_enabled":false,"vibe_tweet_context_enabled":false,'                '"responsive_web_edit_tweet_api_enabled":false,"standardized_nudges_misinfo":false,'                '"responsive_web_enhance_cards_enabled":false}',  
}  
  
DETAIL_URL = 'https://twitter.com/i/api/graphql/0vaSJ4y9SDdSPPZ72dpuDA/TweetDetail'  
  
  
def crawl(tw_id):  
    """  
    å¤–éƒ¨è°ƒç”¨è¿™ä¸ªå‡½æ•°  
    :return:{posts:str,reply:str list}  
    """    tw_id = int(tw_id)  
    raw_json = download(tw_id)  
    if raw_json == -1:  
        return "ERROR"  
    result = extract(raw_json, tw_id)  
    return result  
  
  
def extract(data, tw_id):  
    """  
    è§£æè¿”å›çš„json  
    :param tw_id: æ‰“å°æ—¥å¿—å¥½çœ‹  
    :param data æ¥æ”¶åŸå§‹json  
    :return {posts:str list,reply:str list}    """  
    print("extracting tweets data %s " % tw_id)  
    # å–æ•°æ®ï¼Œæ²¡ä»€ä¹ˆå¥½è¯´çš„ï¼Œç»“æ„é•¿ä»€ä¹ˆæ ·å°±æ€ä¹ˆå–,æœ‰é—®é¢˜å°±æ˜¯jsoné‡Œé¢æœ‰æ²¡æœ‰å€¼  
    posts = []  
    entries = []  
    try:  
        instructions = data["data"]["threaded_conversation_with_injections_v2"]["instructions"]  
    except KeyError as e:  
        print(e)  
        print("ERROR:",tw_id,data)  
        return {'posts': ["key threaded_conversation_with_injections_v2 error"],'reply': []}  
    for i in instructions:  
        if i["type"] == "TimelineAddEntries":  
            entries = i["entries"]  
    post = entries[0]["content"]["itemContent"]["tweet_results"]["result"]["legacy"]["full_text"]  
    posts.append(post)  
    media = entries[0]["content"]["itemContent"]["tweet_results"]["result"]["legacy"].get("extended_entities", "")  
    if media != "":  
        for i in media["media"]:  
            posts.append(i["media_url_https"])  
  
    reply = []  
    for entry in entries[1:-1]:  
        _ = entry["content"]["items"][0]["item"]["itemContent"]["tweet_results"]["result"].get('tombstone', False)  # å¤„ç†å°ç¦è´¦å·çš„æ¶ˆæ¯  
        if _:  
            print("æ¶ˆæ¯æ¥è‡ªå·²å°ç¦è´¦å·")  
            continue  
        _ = entry["content"]["items"][0]["item"]["itemContent"]["tweet_results"]["result"]["legacy"]["full_text"]  
        reply.append(_)  
  
    return {"posts": posts, "reply": reply}  
  
  
def download(tw_id):  
    """  
    :param tw_id: è¯¥æ¡æ¨æ–‡çš„id  
    :return:  
    """    PARAMS = PARAMS_.copy()  
    PARAMS['variables'] = PARAMS['variables'] % tw_id  
    while True:  
        try:  
            response = requests.get(DETAIL_URL, params=PARAMS, cookies=COOKIES, headers=HEADERS)  
        except ProxyError or ConnectionResetError:  
            print("deatil:ConnectionResetError")  
            time.sleep(20)  
            continue  
        break    if response.status_code != 200:  # è¿”å›ç é”™è¯¯å°±å‘ä¸Šä¼ é€’  
        print("error %d when crawing tweets %d " % (response.status_code, tw_id))  
        return -1  
  
    print("status %d crawing tweets %d" % (response.status_code, tw_id))  
    return dict(response.json())  
  
  
if __name__ == "__main__":  
    print(crawl(557649460810248194))  
    print(crawl(899647503859032065))
```
è¿è¡Œç»“æœ
```

```
:::
persistence.pyæŒä¹…åŒ–
::: details Click to see more

```python
# -*-coding:utf-8-*-  
# SettingCode here  
__author__ = "a_little_rubbish"  
__date__ = "2022/7/6 10:00"  
  
from sqlalchemy import Column, String, Integer, Text, create_engine  
from sqlalchemy.ext.declarative import declarative_base  
from sqlalchemy.orm import sessionmaker  
  
Base = declarative_base()  # ç”ŸæˆModelç±»çš„åŸºç±»  
  
  
class Posts(Base):  
    # è¡¨çš„åå­—:  
    __tablename__ = 'posts'  
  
    # è¡¨çš„ç»“æ„:  
    id = Column(Integer, nullable=False, autoincrement=True, primary_key=True)  # èµ„å¢intä¸»é”®  
    main_posts = Column(Text, nullable=False)  # ä¸»æ¨æ–‡  
    reply = Column(Text)  # å›å¤  
  
    def __repr__(self):  
        return 'Posts(id={}, posts={}, reply={})'.format(self.id, self.main_posts, self.reply)  
  
    def __str__(self):  
        return self.__repr__()  
  
  
def init_table():  
    engine = create_engine('mysql+pymysql://root:1118@127.0.0.1:3306/twitter', echo=True)  
    Base.metadata.drop_all(engine)  # åˆ é™¤æ‰€æœ‰è¡¨  
    Base.metadata.create_all(engine)  # åˆ›å»ºæ‰€æœ‰è¡¨  
  
  
def persist_sql(data):  
    """  
    æŒä¹…åŒ–åˆ°sqlçš„å‡½æ•°  
    :param data: jsonæ•°æ®ï¼Œå­˜sql  
    :return:  
    """    engine = create_engine('mysql+pymysql://root:1118@127.0.0.1:3306/twitter', echo=False)  
    Session = sessionmaker(bind=engine)  
    session = Session()  
  
    # æ ¼å¼å˜ä¸€ä¸‹  
    main_posts = ";".join(data["posts"])  
    reply = ";".join(data["reply"])  
  
    print("inserting data", data)  
  
    # æ’å…¥  
    p = Posts(main_posts=main_posts, reply=reply)  
    session.add(p)  
    try:  
        session.commit()  
    except Exception as e:  
        session.rollback()  
        raise e  
  
  
if __name__ == "__main__":  
    # æµ‹è¯•æ•°æ®  
    demodata = {'posts': [  
        '@DamonAlbarn I was such a big fan of yours until I saw this. I write ALL of my own songs. Your hot take is completely false and SO damaging. You donâ€™t have to like my songs but itâ€™s really fucked up to try and discredit my writing. WOW.'],  
        'reply': ['PS I wrote this tweet all by myself in case you were wondering ğŸ˜‘',  
                  '@taylorswift13 I totally agree with you. i had a conversation about songwriting and sadly it was reduced to clickbait. I apologise unreservedly and unconditionally. The last thing I would want to do is discredit your songwriting. I hope you understand. - Damon',  
                  '@taylorswift13 @Damonalbarn ÙŠØ§Ø¨Ù†Øª Ø§Ù„Ø­Ù„Ø§Ù„ Ø§Ø·Ù„Ø¹ÙŠ Ø®Ù„Ø§Øµ ÙˆØ¬Ø¹',  
                  '@taylorswift13 @Damonalbarn Must you use foul language? I understand your point and im a Christian and this just is upsetting to see ladies use such language. Im a great grandmother, grandmother and mother....I like your music just the language is upsetting.',  
                  '@taylorswift13 @Damonalbarn Just gonna leave this here. Look at all of these cowrites @Damonalbarn \n\nâ€œDay doo de bop\nDay doo de bopâ€ really required multiple writers to come up with thatâ€¦ https://t.co/aU8g9bD2rx',  
                  "@taylorswift13 @Damonalbarn what the fuck taylor you can't just say fucking it's rude",  
                  '@taylorswift13 @Damonalbarn OMG I JUST WOKE UP TO SEE TAYLOR SWIFT END A MAN YESSSSS https://t.co/3CAAXx1VGe',  
                  '@taylorswift13 @Damonalbarn https://t.co/Qb8BbHxFSY',  
                  '@taylorswift13 @Damonalbarn heâ€™s just jealous he doesnâ€™t have tons of awards like this shiny songwriter of the year 2020 awardğŸ˜Œ https://t.co/JdwEN18dBT']}  
    init_table()  
    persist_sql(demodata)
```
è¿è¡Œç»“æœ
```
inserting data {'posts': ....

```
:::

::: tip ğŸ“ŒTip
è¿”å›çš„jsonåœ¨å¤„ç†çš„æ—¶å€™æ³¨æ„æ¯æ¬¡éƒ½ä¼šæœ‰ç½®é¡¶æ¨ï¼Œè®°å¾—æ ‡è®°ä¸€ä¸‹ä¸è¦é‡å¤ï¼Œè¿˜æœ‰åˆ¤æ–­è¾¹ç•Œæ¡ä»¶ä»€ä¹ˆæ—¶å€™åœæ­¢è¿›ç¨‹  
æŒä¹…åŒ–çš„æ—¶å€™æ³¨æ„å¤šè¿›ç¨‹çš„å‘ï¼Œæ¯ä¸ªè¿›ç¨‹ä¸€ä¸ªengineï¼Œç”³è¯·sessionè¿™ç±»çš„æ“é”™ä¸è¦æ”¾åœ¨Trueä¸­  
å¤šè¿›ç¨‹çš„å˜é‡åªæœ‰ç¬¬ä¸€å±‚æ˜¯å…±äº«çš„ï¼Œå¤šå±‚åµŒå¥—çš„éœ€è¦ä¸­é—´å˜é‡é—´æ¥èµ‹å€¼æ‰å¯ä»¥ï¼Œä»£ç é‡Œæœ‰
:::

æ•´ä½“ç»„ç»‡
![](./static/twitterçˆ¬è™«_images_6.jpeg)
**æ•°æ®æˆªå›¾**ï¼š
![900](./static/twitterçˆ¬è™«_images_7.png)

### å¤šè¿›ç¨‹
æ•°æ®å¤§è‡´æ˜¯è¿™æ ·ï¼Œç”Ÿäº§è€…1->æ¶ˆè´¹è€…1->æ¶ˆè´¹è€…2ï¼Œé€šè¿‡é˜Ÿåˆ—è¿æ¥è¿›ç¨‹ï¼Œç”Ÿäº§è€…1ä¸€ä¸ªï¼Œæ¶ˆè´¹è€…1ä¸‰ä¸ªï¼Œæ¶ˆè´¹è€…2ä¸‰ä¸ª  
æ­¤æ—¶ç”Ÿäº§è€…1å®Œæˆä¹‹åï¼Œå…ˆæ£€æŸ¥æ•°æ®æ˜¯å¦å®Œå…¨æ”¾åˆ°é˜Ÿåˆ—ä¸­äº†ï¼Œç„¶åæŠŠçŠ¶æ€è¡¨é‡Œè‡ªå·±çš„çŠ¶æ€æ”¹ä¸ºåœæ­¢  
æ¶ˆè´¹è€…1å–é˜Ÿåˆ—å–ä¸åˆ°ä¹‹åç­‰å¾…ä¸‰ç§’ï¼Œtryä½é”™è¯¯ï¼Œæ£€æŸ¥è¡¨ä¸­ç”Ÿäº§è€…1çš„çŠ¶æ€ï¼Œå¦‚æœå…¨å…³é—­äº†é‚£å°±æ˜¯æ•°æ®å–å®Œäº†ï¼Œè‡ªå·±ä¹Ÿå…³é—­ï¼Œæ”¹å˜è¡¨çš„çŠ¶æ€ã€‚  
åç»­æ¶ˆè´¹è€…éƒ½å¦‚æ­¤ã€‚  
è¿™ä¸ªç¨‹åºå¯åœæ˜¯è¿™æ ·è®¾è®¡çš„ã€‚  
### åçˆ¬æœºåˆ¶
æ¨ç‰¹ä¸»è¦å°±æ˜¯é¢‘ç‡é™åˆ¶å’Œcookieï¼Œ15ç§’ä¸€æ¬¡åŠ¨ä¸åŠ¨å°±reset connectionï¼Œéœ€è¦tryä¹‹åç­‰å¾…å†å»è¯·æ±‚ï¼Œä¸è¦è®©è¿›ç¨‹é‡Œå‡ºç°é”™è¯¯ï¼Œå¦åˆ™è¿›ç¨‹å°±æŒ‚äº†  
å†å°±æ˜¯è¯·æ±‚å¤´å’Œcookieï¼Œéƒ½è¦å¸¦ä¸Šï¼Œä¸ç„¶å°±è¿‡ä¸å»  


## é—®é¢˜
é”™è¯¯å¤„ç†ï¼Œæ—¥å¿—æ–¹é¢è¿˜ä¸å®Œç¾ï¼Œå‡ºç°çš„é”™è¯¯éƒ½æ˜¯ç›´æ¥tryåˆ°äº†ä¹‹åï¼Œæ‰“å°ä¿¡æ¯è·³è¿‡ï¼Œç»§ç»­ä¸‹ä¸€æ¬¡ï¼Œæ²¡æœ‰è®¾è®¡è¶³å¤Ÿçš„æ’æŸ¥ä¿¡æ¯  
æ—¥å¿—æ²¡ç”¨loggerï¼ŒçŠ¶æ€ç®¡ç†éƒ½æ˜¯å®æ—¶æ‰“å°ï¼Œå¾ˆå¤šå†—ä½™ä¿¡æ¯ï¼Œä¹Ÿä¸çŸ¥é“åº”è¯¥æ‰“å°å“ªäº›ä¿¡æ¯  
çº¿ç¨‹çŠ¶æ€ç®¡ç†åº”è¯¥é…åˆé”™è¯¯å¤„ç†  

è§£ææ•°æ®çš„æ—¶å€™å…¶å®æˆ‘æ˜¯ç›´æ¥å†™æ­»çš„æ‹¿keyï¼Œå¾ˆä¸çµæ´»ï¼Œç¢°åˆ°çš„é—®é¢˜æœ‰ å·²ç»å°ç¦è´¦å·å‘æ¶ˆæ¯ï¼Œä¸çŸ¥é“å“ªäº›è¯·æ±‚æ²¡æœ‰æ•°æ®ï¼Œä¼šæŠ¥key errorï¼Œå†™ä¸ªtryåšä¸€ä¸‹å¤„ç†ï¼Œè¿˜æœ‰ä¸çŸ¥é“è¿™äº›jsoné‚£ä¸ªkeyæ²¡è·å–åˆ°çš„ï¼Œæ²¡æœ‰è¯„è®ºæ•°æ®ï¼Œä¸è¿‡è¿™ç§ä¸œè¥¿éƒ½ä¸æ˜¯å¿…é¡»çš„äº†ï¼Œæ•°æ®ä¸‹æ¥äº†æƒ³è¦å°±æ˜¯æ—©æ™šçš„äº‹ï¼Œè¿™äº›ä¹Ÿä¸æ˜¯é‡ç‚¹ã€‚
::: info ğŸ“„Info
çˆ¬è™«æœ‰å®æ•ˆæ€§ï¼Œç½‘ç«™ä¸€å˜çˆ¬è™«åŸºæœ¬å°±è·Ÿç€å˜
:::


## ç¬¬äºŒç‰ˆv2
ä¸æ˜¯demoäº†ï¼Œå¾—æ‹¿ä¸Šå»éƒ¨ç½²ï¼Œå¾ˆå¤šæ¨¡å—éƒ½å¾—é‡æ–°è®¾è®¡ã€‚  

